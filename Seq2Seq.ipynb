{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Iv0GCK-MDO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70d8e63-279e-4d1c-d5e7-9ac7061910da"
      },
      "source": [
        "!python -m spacy download fr\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torchtext.datasets import Multi30k\r\n",
        "from torchtext.data import Field, BucketIterator\r\n",
        "import numpy as np\r\n",
        "import spacy\r\n",
        "import random\r\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fr_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.5/fr_core_news_sm-2.2.5.tar.gz (14.7MB)\n",
            "\u001b[K     |████████████████████████████████| 14.7MB 23.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from fr_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.8.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (51.3.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.4.0)\n",
            "Building wheels for collected packages: fr-core-news-sm\n",
            "  Building wheel for fr-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fr-core-news-sm: filename=fr_core_news_sm-2.2.5-cp36-none-any.whl size=14727027 sha256=afbcfd6250fde6e6c574cdb3646fbec9a87643de83d3f36a1f8e3ab216ef82fd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-35_su425/wheels/46/1b/e6/29b020e3f9420a24c3f463343afe5136aaaf955dbc9e46dfc5\n",
            "Successfully built fr-core-news-sm\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/fr_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/fr\n",
            "You can now load the model via spacy.load('fr')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PR6o-r_TOGMn",
        "outputId": "5402e3f3-78dc-4afa-ffcc-f9dc8a311aa3"
      },
      "source": [
        "!ls .data/multi30k\r\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz  test2016.fr\ttraining.tar.gz  validation.tar.gz\n",
            "test2016.de\t\t   train.de\tval.de\n",
            "test2016.en\t\t   train.en\tval.en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66ceMgsvMRNd"
      },
      "source": [
        "nlp_de = spacy.load('de')\r\n",
        "nlp_eng = spacy.load('en')\r\n",
        "\r\n",
        "def tokenizer_de(text):\r\n",
        "    return [tok.text for tok in nlp_de.tokenizer(text)]\r\n",
        "\r\n",
        "\r\n",
        "def tokenizer_eng(text):\r\n",
        "    return [tok.text for tok in nlp_eng.tokenizer(text)]\r\n",
        "\r\n",
        "\r\n",
        "german = Field(tokenize = tokenizer_de, lower=True, init_token ='<sos>', eos_token = '<eos>')\r\n",
        "\r\n",
        "english = Field(tokenize = tokenizer_eng, lower=True, init_token ='<sos>', eos_token = '<eos>')\r\n",
        "\r\n",
        "train_data, validation_data, test_data = Multi30k.splits(exts = ('.de', '.en'), fields = (german, english))\r\n",
        "\r\n",
        "german.build_vocab(train_data, max_size = 10000, min_freq = 2) \r\n",
        "english.build_vocab(train_data, max_size = 10000, min_freq = 2) "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfUx7D_cNeJY"
      },
      "source": [
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train_data, validation_data, test_data),\r\n",
        "    batch_size = 3,\r\n",
        "    sort_within_batch = True,\r\n",
        "    sort_key = lambda x: len(x.src))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uedxAfxqNnKE",
        "outputId": "cd44a65b-4357-46e9-f686-e8f9767f40dd"
      },
      "source": [
        "x = next(iter(train_iterator))\r\n",
        "print(x.src)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[   2,    2,    2],\n",
            "        [ 105,   18,   18],\n",
            "        [  41,   30,   41],\n",
            "        [  52,    7,   53],\n",
            "        [  27,  486,   10],\n",
            "        [   6, 3534,  307],\n",
            "        [3487,   59,  127],\n",
            "        [  10,    6,  317],\n",
            "        [ 928,    0, 3093],\n",
            "        [  75,  292, 3542],\n",
            "        [   4,    4,    4],\n",
            "        [   3,    3,    3]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_lypgEUL6gU"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\r\n",
        "        super(Encoder, self).__init__()\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.num_layers = num_layers\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(p)\r\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\r\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        # c_shape (seq_length, batch_size, )\r\n",
        "        embedding = self.dropout(self.embedding(x))\r\n",
        "        # embedding (seq_length, batch_size, embedding_size)\r\n",
        "        outputs, (hidden, cell) = self.rnn(embedding)\r\n",
        "\r\n",
        "        return hidden, cell\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\r\n",
        "        super(Decoder, self).__init__()\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.num_layers = num_layers\r\n",
        "        self.dropout = nn.Dropout(p)\r\n",
        "\r\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\r\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\r\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\r\n",
        "\r\n",
        "    \r\n",
        "    def forward(self, x, hidden, cell):\r\n",
        "        \r\n",
        "        # shape of x (batch_size) but we want (1,batch_size) \r\n",
        "        x = x.unsqueeze(0)\r\n",
        "        embedding = self.dropout(self.embedding(x))\r\n",
        "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\r\n",
        "        # shape of outputs (1, N, hidden_size)\r\n",
        "\r\n",
        "        predictions = self.fc(outputs)\r\n",
        "        # shape 1, N, length of vocab\r\n",
        "\r\n",
        "        predictions = predictions.squeeze(0)\r\n",
        "\r\n",
        "        return predictions, hidden, cell\r\n",
        "\r\n",
        "class Seq2Seq(nn.Module):\r\n",
        "    def __init__(self, encoder, decoder):\r\n",
        "        super(Seq2Seq, self).__init__()\r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, source, target, teacher_force_ratio = 0.5):\r\n",
        "        batch_size = source.shape[1]\r\n",
        "        target_len = target.shape[0]\r\n",
        "        target_vocab_size = len(english.vocab)\r\n",
        "\r\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\r\n",
        "\r\n",
        "        hidden, cell = self.encoder(source)\r\n",
        "        \r\n",
        "    \r\n",
        "        #grab start toker\r\n",
        "        x = target[0]\r\n",
        "\r\n",
        "        for t in range(1, target_len):\r\n",
        "            output, hidden, cell = self.decoder(x, hidden, cell)\r\n",
        "\r\n",
        "            outputs[t] = output\r\n",
        "\r\n",
        "            # (N, english_vocab_size)\r\n",
        "            best_guess = output.argmax(1)\r\n",
        "            x = target[t] if random.random() < teacher_force_ratio else best_guess\r\n",
        "\r\n",
        "\r\n",
        "        return outputs\r\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGn73VsfSs7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8043fb95-a56f-4b77-a91e-c2efa634939c"
      },
      "source": [
        "\r\n",
        "### Training\r\n",
        "\r\n",
        "# training hyperparameters\r\n",
        "\r\n",
        "num_epochs = 5\r\n",
        "learning_rate = 0.001\r\n",
        "batch_size = 32\r\n",
        "\r\n",
        "# Model hyperparameters\r\n",
        "load_model = False\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "input_size_encoder = len(german.vocab)\r\n",
        "input_size_decoder = len(english.vocab)\r\n",
        "output_size = len(english.vocab)\r\n",
        "encoder_embedding_size = 300\r\n",
        "decoder_embedding_size = 300\r\n",
        "hidden_size = 1024\r\n",
        "num_layers = 1\r\n",
        "enc_dropout = 0.5\r\n",
        "dec_dropout = 0.5\r\n",
        "\r\n",
        "#Tensor board\r\n",
        "writer = SummaryWriter()\r\n",
        "step = 0\r\n",
        "\r\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train_data, validation_data, test_data),\r\n",
        "    batch_size = batch_size,\r\n",
        "    sort_within_batch = True,\r\n",
        "    sort_key = lambda x: len(x.src),\r\n",
        "    device = device\r\n",
        ")\r\n",
        "\r\n",
        "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout).to(device)\r\n",
        "\r\n",
        "decoder_net = Decoder(input_size_decoder, decoder_embedding_size, hidden_size, output_size, num_layers, dec_dropout).to(device)\r\n",
        "\r\n",
        "model = Seq2Seq(encoder_net, decoder_net).to(device)\r\n",
        "\r\n",
        "pad_idx = english.vocab.stoi['<pad>']\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\r\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "id": "eTjlKHCwSvxx",
        "outputId": "af9d05f2-baf9-4d1c-fb09-f6f45b835063"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\r\n",
        "\r\n",
        "# saving stuff\r\n",
        "\r\n",
        "v_inp_data, v_tgt_data = next(iter(valid_iterator)).src,  next(iter(valid_iterator)).trg\r\n",
        "\r\n",
        "i = 0\r\n",
        "\r\n",
        "for epoch in range(100):\r\n",
        "    print(f'Epoch {epoch} / {num_epochs}')\r\n",
        "\r\n",
        "    checkpoint = {'state_dict':model.state_dict(), 'optimizer':optimizer.state_dict()}\r\n",
        "\r\n",
        "    # save_checkpoint(checkpoint)\r\n",
        "    a = translate_sentence(model, 'Der Himmel ist heute klar. Die Sonne scheint.', german, english, device)\r\n",
        "    print(a)\r\n",
        "    for batch_idx, batch in enumerate(train_iterator):\r\n",
        "        inp_data = batch.src.to(device)\r\n",
        "        target = batch.trg.to(device)\r\n",
        "\r\n",
        "        output = model(inp_data, target)\r\n",
        "        #output shape (tgt_len ,b_size, output_dim)\r\n",
        "\r\n",
        "        if i == 0:\r\n",
        "            writer.add_graph(model.encoder, inp_data)\r\n",
        "            i += 1\r\n",
        "\r\n",
        "        output = output[1:].reshape(-1, output.shape[2])\r\n",
        "        target = target[1:].reshape(-1)\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss = criterion(output, target)\r\n",
        "\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1)\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        writer.add_scalar('/Training Loss/Tr_Loss', loss, global_step = step)\r\n",
        "\r\n",
        "        #val loss\r\n",
        "        v_output = model(v_inp_data, v_tgt_data)\r\n",
        "        #output shape (tgt_len ,b_size, output_dim)\r\n",
        "\r\n",
        "        v_output = v_output[1:].reshape(-1, v_output.shape[2])\r\n",
        "        v_target = v_tgt_data[1:].reshape(-1)\r\n",
        "\r\n",
        "        val_loss = criterion(v_output, v_target)\r\n",
        "        writer.add_scalar('/Validation Loss/Val_loss', val_loss, global_step = step)\r\n",
        "\r\n",
        "        step += 1\r\n",
        "\r\n",
        "writer.close()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 / 5\n",
            "['attaching', 'attaching', 'spilled', 'taking', 'fixing', '100', 'attaching', 'shelves', 'sprinkles', 'completing', 'smoking', 'competition', 'balding', 'someones', 'showgirl', 'image', 'arranged', 'satchel', 'much', 'wheat', 'sing', 'organized', 'kind', 'ballerinas', 'ballerinas', 'bloom', 'repairing', 'waits', 'swans', 'taste', 'studies', 'lollipop', 'observed', 'caricature', 'cream', 'ribs', 'ribs', 'tattoos', 'operated', 'posed', 'close', 'lots', 'cue', 'weave', 'dirty', 'milk', 'monitor', 'carpenter', 'products', 'gesture']\n",
            "Epoch 1 / 5\n",
            "['the', '<unk>', 'is', '<unk>', 'the', 'the', 'the', '<unk>', '.', '<eos>']\n",
            "Epoch 2 / 5\n",
            "['the', '<unk>', '<unk>', 'is', 'the', 'the', 'the', 'the', '.', '.', '<eos>']\n",
            "Epoch 3 / 5\n",
            "['the', '<unk>', 'is', 'the', 'the', 'the', 'the', 'the', '.', '.', '<eos>']\n",
            "Epoch 4 / 5\n",
            "['the', '<unk>', \"'s\", '<unk>', 'is', 'the', 'the', 'the', 'the', '.', '.', '<eos>']\n",
            "Epoch 5 / 5\n",
            "['the', '<unk>', 'is', 'is', 'the', 'the', 'of', 'the', 'red', 'and', 'white', '.', '<eos>']\n",
            "Epoch 6 / 5\n",
            "['the', 'ambulance', 'is', 'is', '<unk>', 'the', 'the', 'the', '.', '<eos>']\n",
            "Epoch 7 / 5\n",
            "['the', '<unk>', '<unk>', 'is', '<unk>', 'the', 'the', 'the', 'the', '.', '<eos>']\n",
            "Epoch 8 / 5\n",
            "['the', '<unk>', 'is', '<unk>', 'is', 'the', 'the', 'the', 'of', '.', '<eos>']\n",
            "Epoch 9 / 5\n",
            "['the', 'car', 'is', 'the', '<unk>', 'of', 'the', 'the', '.', '<eos>']\n",
            "Epoch 10 / 5\n",
            "['the', '<unk>', '<unk>', 'was', '<unk>', 'sun', 'during', 'the', 'sun', '.', '<eos>']\n",
            "Epoch 11 / 5\n",
            "['the', '<unk>', 'is', 'the', '<unk>', '<unk>', 'the', 'sun', '.', '<eos>']\n",
            "Epoch 12 / 5\n",
            "['the', '<unk>', '<unk>', 'is', 'is', 'is', 'is', 'is', 'is', '.', '<eos>']\n",
            "Epoch 13 / 5\n",
            "['the', '<unk>', '<unk>', 'is', '<unk>', 'by', 'the', 'sun', '.', '<eos>']\n",
            "Epoch 14 / 5\n",
            "['the', '<unk>', 'is', 'was', '<unk>', 'the', 'the', 'the', '.', '.', '<eos>']\n",
            "Epoch 15 / 5\n",
            "['the', '<unk>', 'is', 'the', '<unk>', 'big', 'sun', '.', '<eos>']\n",
            "Epoch 16 / 5\n",
            "['the', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', '<eos>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-8618487805de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m#output shape (tgt_len ,b_size, output_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-5fadd83bcdad>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, source, target, teacher_force_ratio)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mtarget_vocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FVSNXoHOtwI"
      },
      "source": [
        "def translate_sentence(model, sentence, german, english, device, max_length=50):\r\n",
        "    # Load german tokenizer\r\n",
        "    spacy_ger = spacy.load(\"de\")\r\n",
        "\r\n",
        "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\r\n",
        "    if type(sentence) == str:\r\n",
        "        tokens = [token.text.lower() for token in spacy_ger(sentence)]\r\n",
        "    else:\r\n",
        "        tokens = [token.lower() for token in sentence]\r\n",
        "\r\n",
        "    # print(tokens)\r\n",
        "\r\n",
        "    # sys.exit()\r\n",
        "    # Add <SOS> and <EOS> in beginning and end respectively\r\n",
        "    tokens.insert(0, german.init_token)\r\n",
        "    tokens.append(german.eos_token)\r\n",
        "\r\n",
        "    # Go through each german token and convert to an index\r\n",
        "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\r\n",
        "\r\n",
        "    # Convert to Tensor\r\n",
        "    sentence_tensor = torch.tensor(text_to_indices).unsqueeze(1).to(device)\r\n",
        "\r\n",
        "    # Build encoder hidden, cell state\r\n",
        "    with torch.no_grad():\r\n",
        "        hidden, cell = model.encoder(sentence_tensor)\r\n",
        "\r\n",
        "    outputs = [english.vocab.stoi[\"<sos>\"]]\r\n",
        "\r\n",
        "    for _ in range(max_length):\r\n",
        "        previous_word = torch.tensor([outputs[-1]]).to(device)\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "            output, hidden, cell = model.decoder(previous_word, hidden, cell)\r\n",
        "            best_guess = output.argmax(1).item()\r\n",
        "\r\n",
        "        outputs.append(best_guess)\r\n",
        "\r\n",
        "        # Model predicts it's the end of the sentence\r\n",
        "        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\r\n",
        "            break\r\n",
        "\r\n",
        "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\r\n",
        "\r\n",
        "    # remove start token\r\n",
        "    return translated_sentence[1:]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqs6G6NyPMoO",
        "outputId": "d1f5aa4c-9487-4311-e43c-17625c06a970"
      },
      "source": [
        "translate_sentence(model, 'Germany', german, english, device)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['school', 'children', 'going', 'on', 'a', 'ground', '.', '<eos>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_BGN9HAYV2B"
      },
      "source": [
        "%load_ext tensorboard\r\n",
        "%tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sESEvWaBMW9f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}